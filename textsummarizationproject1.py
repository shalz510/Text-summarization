# -*- coding: utf-8 -*-
"""Textsummarizationproject1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18F9EK9Wi9PTE-RVE1cuAZrbfDfT_Jl48
"""

import os
import re
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import pipeline
from pdfminer.high_level import extract_text

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# 1. Dataset Preprocessing
def extract_text_from_pdf(file_path):
    """Extracts text from a PDF file."""
    return extract_text(file_path)

def clean_text(text):
    """Removes special characters and normalizes text."""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^a-zA-Z0-9.\?\!\s]', '', text)
    return text.lower()

# 2. Chunking and Feature Extraction
def split_into_chunks(text, chunk_size=1000):
    """Splits the text into chunks of approximately the specified size."""
    words = text.split()
    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

def get_important_sentences(text, top_n=10):
    """Identifies important sentences using TF-IDF."""
    sentences = sent_tokenize(text)
    vectorizer = TfidfVectorizer(stop_words='english')
    vectors = vectorizer.fit_transform(sentences)
    scores = vectors.sum(axis=1).flatten().tolist()
    sentence_scores = [(score, sentence) for score, sentence in zip(scores, sentences)]
    sentence_scores.sort(key=lambda x: x[0], reverse=True)
    summary = [sentence for _, sentence in sentence_scores[:top_n]]  # Top N sentences
    return ' '.join(summary)

# 3. Abstractive Summarization
def abstractive_summarizer(text, model_name='facebook/bart-large-cnn', max_chunk_size=512):
    """Summarizes text using a pre-trained transformer model."""
    summarizer = pipeline("summarization", model=model_name, device=0)  # Use CPU or GPU
    sentences = sent_tokenize(text)

    # Split sentences into chunks fitting within the model's token limit
    chunks = []
    current_chunk = []
    current_length = 0

    for sentence in sentences:
        sentence_length = len(sentence.split())
        if current_length + sentence_length <= max_chunk_size:
            current_chunk.append(sentence)
            current_length += sentence_length
        else:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_length = sentence_length

    if current_chunk:  # Add any remaining sentences as the last chunk
        chunks.append(" ".join(current_chunk))

    # Summarize each chunk and combine results
    summaries = []
    for i, chunk in enumerate(chunks):
        print(f"Processing Chunk {i + 1}: {chunk[:200]}...")
        try:
            summary = summarizer(chunk, max_length=300, min_length=100, do_sample=False)
            summaries.append(summary[0]['summary_text'])
        except Exception as e:
            print(f"Error in chunk {i + 1}: {e}")
            summaries.append(f"Error summarizing chunk {i + 1}: {e}")

    return " ".join(summaries)

# Main Execution
def main():
    pdf_file_path = "507 Submission.pdf"  # Replace with your file path

    # Step 1: Extract and Clean Text
    raw_text = extract_text_from_pdf(pdf_file_path)
    cleaned_text = clean_text(raw_text)

    # Step 2: Chunk and Summarize
    chunks = split_into_chunks(cleaned_text, chunk_size=1500)
    extractive_summary = []
    abstractive_summary = []

    for i, chunk in enumerate(chunks):
        print(f"\nProcessing Chunk {i + 1}:\n")
        extractive_summary.append(get_important_sentences(chunk, top_n=15))  # Extract 15 sentences per chunk
        abstractive_summary.append(abstractive_summarizer(chunk))
        print("="*50)
        print(f"Chunk {i+1}:")
        print(f"Word Count: {len(chunk.split())}")
        print(f"Extractive Summary:\n{extractive_summary[-1]}")
        print(f"Abstractive Summary:\n{abstractive_summary[-1]}")
        print("="*50)


    # Combine Extractive and Abstractive Summaries
    combined_summary = "\n".join([
        f"Extractive Summary (Chunk {i + 1}):\n{extractive}\n\nAbstractive Summary (Chunk {i + 1}):\n{abstract}\n"
        for i, (extractive, abstract) in enumerate(zip(extractive_summary, abstractive_summary))
    ])

    # Print the final summary
    print("\nFinal Combined Summary:\n")
    print(combined_summary)

if __name__ == "__main__":
    main()

!pip install pdfminer.six

import nltk
nltk.download('punkt_tab')

import os
import re
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import pipeline
from pdfminer.high_level import extract_text

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# 1. Dataset Preprocessing
def extract_text_from_pdf(file_path):
    """Extracts text from a PDF file."""
    return extract_text(file_path)

def clean_text(text):
    """Removes special characters and normalizes text."""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^a-zA-Z0-9.\?\!\s]', '', text)
    return text.lower()

# 2. Chunking and Feature Extraction
def split_into_chunks(text, chunk_size=1000):
    """Splits the text into chunks of approximately the specified size."""
    words = text.split()
    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

def get_important_sentences(text, top_n=10):
    """Identifies important sentences using TF-IDF."""
    sentences = sent_tokenize(text)
    vectorizer = TfidfVectorizer(stop_words='english')
    vectors = vectorizer.fit_transform(sentences)
    scores = vectors.sum(axis=1).flatten().tolist()
    sentence_scores = [(score, sentence) for score, sentence in zip(scores, sentences)]
    sentence_scores.sort(key=lambda x: x[0], reverse=True)
    summary = [sentence for _, sentence in sentence_scores[:top_n]]  # Top N sentences
    return ' '.join(summary)

# 3. Abstractive Summarization
def abstractive_summarizer(text, model_name='facebook/bart-large-cnn', max_chunk_size=512):
    """Summarizes text using a pre-trained transformer model."""
    summarizer = pipeline("summarization", model=model_name, device=0)  # Use CPU or GPU
    sentences = sent_tokenize(text)

    # Split sentences into chunks fitting within the model's token limit
    chunks = []
    current_chunk = []
    current_length = 0

    for sentence in sentences:
        sentence_length = len(sentence.split())
        if current_length + sentence_length <= max_chunk_size:
            current_chunk.append(sentence)
            current_length += sentence_length
        else:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_length = sentence_length

    if current_chunk:  # Add any remaining sentences as the last chunk
        chunks.append(" ".join(current_chunk))

    # Summarize each chunk and combine results
    summaries = []
    for i, chunk in enumerate(chunks):
        print(f"Processing Chunk {i + 1}: {chunk[:200]}...")
        try:
            summary = summarizer(chunk, max_length=300, min_length=100, do_sample=False)
            summaries.append(summary[0]['summary_text'])
        except Exception as e:
            print(f"Error in chunk {i + 1}: {e}")
            summaries.append(f"Error summarizing chunk {i + 1}: {e}")

    return " ".join(summaries)

# Main Execution
#def main():
pdf_file_path = "507 Submission                                                                   .pdf"  # Replace with your file path

# Step 1: Extract and Clean Text
raw_text = extract_text_from_pdf(pdf_file_path)
cleaned_text = clean_text(raw_text)

# Step 2: Chunk and Summarize
chunks = split_into_chunks(cleaned_text, chunk_size=1500)
extractive_summary = []
abstractive_summary = []

for i, chunk in enumerate(chunks):
    print(f"\nProcessing Chunk {i + 1}:\n")
    extractive_summary.append(get_important_sentences(chunk, top_n=15))
    abstractive_summary.append(abstractive_summarizer(chunk))
    print("="*50)
    print(f"Chunk {i+1}:")
    print(f"Word Count: {len(chunk.split())}")
    print(f"Extractive Summary:\n{extractive_summary[-1]}")
    print(f"Abstractive Summary:\n{abstractive_summary[-1]}")
    print("="*50)

# Combine Extractive and Abstractive Summaries into Paragraph
combined_summary = " ".join([
    f"{extractive} {abstract}"
    for extractive, abstract in zip(extractive_summary, abstractive_summary)
])

# Print the final summary as a paragraph
print("\nFinal Combined Summary:\n")
print(combined_summary)
print("="*50)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def generate_wordcloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=16)
    plt.show()

# Generate word clouds for original text and summaries
generate_wordcloud(cleaned_text, "Original Text")
generate_wordcloud(" ".join(extractive_summary), "Extractive Summary")
generate_wordcloud(" ".join(abstractive_summary), "Abstractive Summary")
from IPython.display import display, HTML

def display_summary_table(chunks, extractive_summaries, abstractive_summaries):
    table = "<table><tr><th>Chunk</th><th>Extractive Summary</th><th>Abstractive Summary</th></tr>"
    for i, (chunk, ext_sum, abs_sum) in enumerate(zip(chunks, extractive_summaries, abstractive_summaries)):
        table += f"<tr><td>Chunk {i+1}</td><td>{ext_sum}</td><td>{abs_sum}</td></tr>"
    table += "</table>"
    display(HTML(table))

# Display summaries in a table
display_summary_table(chunks, extractive_summary, abstractive_summary)
import matplotlib.pyplot as plt

# Token counts per chunk
token_counts = [len(chunk.split()) for chunk in chunks]

plt.bar(range(1, len(token_counts) + 1), token_counts, color='skyblue')
plt.xlabel("Chunk Number")
plt.ylabel("Token Count")
plt.title("Token Count per Chunk")
plt.show()
!pip install streamlit
import streamlit as st

st.title("Text Summarization Tool")
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")

if uploaded_file:
    raw_text = extract_text_from_pdf(uploaded_file)
    cleaned_text = clean_text(raw_text)
    chunks = split_into_chunks(cleaned_text, chunk_size=1500)

    extractive_summary = [get_important_sentences(chunk, top_n=10) for chunk in chunks]
    abstractive_summary = [abstractive_summarizer(chunk) for chunk in chunks]

    st.write("## Extractive Summaries")
    for i, summary in enumerate(extractive_summary):
        st.write(f"**Chunk {i+1}:** {summary}")

    st.write("## Abstractive Summaries")
    for i, summary in enumerate(abstractive_summary):
        st.write(f"**Chunk {i+1}:** {summary}")


#if __name__ == "__main__":
#    main()